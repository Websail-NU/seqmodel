{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "sys.path.insert(0, '../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from seqmodel.bunch import Bunch\n",
    "from seqmodel.experiment.policy_agent import PolicyAgent\n",
    "from seqmodel import model\n",
    "from seqmodel import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = data.Vocabulary.from_vocab_file('../data/tiny_copy/vocab.txt')\n",
    "valid_iter = data.Seq2SeqIterator(vocab, vocab)\n",
    "valid_iter.initialize('../data/tiny_copy/valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy_agent/policy/model/encoder_embedding:0, (15, 64)\n",
      "policy_agent/policy/model/decoder_embedding:0, (15, 64)\n",
      "policy_agent/policy/model/encoder_rnn/rnn/basic_lstm_cell/weights:0, (128, 256)\n",
      "policy_agent/policy/model/encoder_rnn/rnn/basic_lstm_cell/biases:0, (256,)\n",
      "policy_agent/policy/model/decoder_rnn/rnn/basic_lstm_cell/weights:0, (128, 256)\n",
      "policy_agent/policy/model/decoder_rnn/rnn/basic_lstm_cell/biases:0, (256,)\n",
      "policy_agent/policy/model/decoder_rnn/logit_w:0, (15, 64)\n",
      "policy_agent/policy/model/decoder_rnn/logit_b:0, (15,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m[INFO ]\u001b[0mep: 0, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 15.22850, eval_loss: 2.28695, wps: 1058.7\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 2.04570, wps: 2498.5\n",
      "\u001b[36m[INFO ]\u001b[0mep: 1, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 12.08141, eval_loss: 1.80471, wps: 1090.5\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 1.38181, wps: 2170.1\n",
      "\u001b[36m[INFO ]\u001b[0mep: 2, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 8.32639, eval_loss: 1.24369, wps: 956.7\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.98122, wps: 2220.7\n",
      "\u001b[36m[INFO ]\u001b[0mep: 3, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 5.29155, eval_loss: 0.78933, wps: 1315.0\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.53771, wps: 2631.0\n",
      "\u001b[36m[INFO ]\u001b[0mep: 4, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 3.60067, eval_loss: 0.53304, wps: 1151.0\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.49340, wps: 4286.7\n",
      "\u001b[36m[INFO ]\u001b[0mep: 5, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 2.65915, eval_loss: 0.39169, wps: 1188.7\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.29793, wps: 2356.5\n",
      "\u001b[36m[INFO ]\u001b[0mep: 6, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 1.90644, eval_loss: 0.28242, wps: 1085.2\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.25623, wps: 2278.9\n",
      "\u001b[36m[INFO ]\u001b[0mep: 7, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 1.48093, eval_loss: 0.21979, wps: 1118.3\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.18773, wps: 2213.7\n",
      "\u001b[36m[INFO ]\u001b[0mep: 8, lr: 0.300000\n",
      "\u001b[36m[INFO ]\u001b[0mtrain: @99 tr_loss: 1.24077, eval_loss: 0.18328, wps: 1100.2\n",
      "\u001b[36m[INFO ]\u001b[0mvalid: @99 tr_loss: 0.00000, eval_loss: 0.16916, wps: 2317.7\n",
      "\u001b[36m[INFO ]\u001b[0mep: 9, lr: 0.300000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PPL: 0.117503514885, time: 2.15334415436\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "agent_opt = PolicyAgent.default_opt()\n",
    "emb_opt = agent_opt.policy_model.model_opt.embedding\n",
    "dec_opt = agent_opt.policy_model.model_opt.decoder\n",
    "enc_opt = agent_opt.policy_model.model_opt.encoder\n",
    "optim_opt = agent_opt.optim\n",
    "\n",
    "emb_opt.decoder_dim = 64\n",
    "emb_opt.encoder_dim = 64\n",
    "\n",
    "dec_opt.rnn_opt.rnn_cell.cell_opt.num_units = 64\n",
    "enc_opt.rnn_opt.rnn_cell.cell_opt.num_units = 64\n",
    "\n",
    "optim_opt.learning_rate = 0.3\n",
    "optim_opt.name = 'GradientDescentOptimizer'\n",
    "\n",
    "sess = tf.Session()\n",
    "agent = PolicyAgent(agent_opt, sess)\n",
    "agent.initialize_model(with_training=True)\n",
    "agent.initialize_optim()\n",
    "for v in tf.trainable_variables():\n",
    "    print('{}, {}'.format(v.name, v.get_shape()))\n",
    "sess.run(tf.global_variables_initializer())\n",
    "agent.train(valid_iter, 10, valid_iter, 10, verbose=True)\n",
    "info = agent.evaluate(valid_iter, 10)\n",
    "print(\"PPL: {}, time: {}\".format(\n",
    "    info.eval_cost/info.num_tokens, info.end_time - info.start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = ([['d e f', 'd e f'], ['a b c d', 'a b c d']])\n",
    "test_iter = data.Seq2SeqIterator(vocab, vocab)\n",
    "test_iter.initialize(test_data)\n",
    "test_iter.init_batch(2)\n",
    "env = data.env.CopyEnv(test_iter, re_init=False, reward_mode=data.env.ToyRewardMode.EACH_MATCH)\n",
    "transitions, packed_t, packed_r = agent.rollout(env, greedy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<seqmodel.experiment.run_info.RunningInfo at 0x7f62a69902d0>, 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_iter.init_batch(2)\n",
    "env = data.env.CopyEnv(test_iter, re_init=False, reward_mode=data.env.ToyRewardMode.EACH_MATCH)\n",
    "agent.evaluate_policy(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "test_iter.init_batch(2)\n",
    "env = data.env.CopyEnv(test_iter, re_init=False, reward_mode=data.env.ToyRewardMode.EACH_MATCH)\n",
    "_ = agent.policy_gradient(env, 2, env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
